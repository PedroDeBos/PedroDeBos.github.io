<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Free space | Portfolio Pedro de Bos" />
<meta property="og:type" content="book" />







<meta name="description" content="Free space | Portfolio Pedro de Bos">

<title>Free space | Portfolio Pedro de Bos</title>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation-1.1/tabsets.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.22/datatables.js"></script>
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if boostrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<!--bookdown:toc:end-->
<!--bookdown:toc:start-->
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="free-space" class="section level1">
<h1>Free space</h1>
<body id="start">
<div class="topnav">
<p><a href='index.html#frontpage'>Front page</a>
<a href='data-visualisation.html#data-visualisation'>Data visualisation</a>
<a href='sorting-directories.html#sorting-directories'>Directory structure</a>
<a href='sql.html#SQL'>SQL</a>
<a href='r-packages.html#R-packages'>R-package</a>
<a href='bibliography-using-zotero.html#Bibliography using Zotero'>Zotero</a>
<a href='reproductibility.html#Reproductibility'>Reproductibility</a>
<a href='cv.html#cv'>CV</a>
<a href='plan-for-future.html#plan-for-future'>The future</a></p>
</div>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="free-space.html#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb55-2"><a href="free-space.html#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
<p>For my free Space, I will study <a href="https://towardsdatascience.com/recognition-and-counting-of-microorganisms-on-petri-dishes-53a499283dc8">this site</a>. in order to find information about using bacterial identification using machine learning.</p>
<p>In this article, there is a link too the site <a href="https://agar.neurosys.com/">agar.neurosys.com</a> which contains a dataset containing picture of agarplates in different conditions</p>
<p>Conclusion based on the site: this is <em>way</em> too complicated to start out with. Trying to find a simpler introduction to machine learning, specifically in R</p>
<p>Trying <a href="https://www.datacamp.com/tutorial/machine-learning-in-r">this link</a>, mayhaps this’ll give a better introduction.</p>
<p>Would appear that a technique called “KNN” might be a good algorythm to start with. Gonna follow the tutorial the previous link gives, using the “iris” dataset</p>
<p>First steps are too really undersand your data, and too visualise your data. So:</p>
<ul>
<li><p>Sepals are modified leaves which encase and protect the flower before it blooms</p></li>
<li><p>Petals are modified leaves that surround the reproductive parts of the flower.</p></li>
</ul>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="free-space.html#cb56-1" aria-hidden="true" tabindex="-1"></a>iris <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>Sepal.Length, <span class="at">y=</span>Sepal.Width, <span class="at">colour=</span>Species))<span class="sc">+</span></span>
<span id="cb56-2"><a href="free-space.html#cb56-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="free-space.html#cb57-1" aria-hidden="true" tabindex="-1"></a>iris <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>Petal.Length, <span class="at">y=</span>Petal.Width, <span class="at">colour=</span>Species))<span class="sc">+</span></span>
<span id="cb57-2"><a href="free-space.html#cb57-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-23-2.png" width="672" /></p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="free-space.html#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(iris)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="free-space.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(iris<span class="sc">$</span>Species)</span></code></pre></div>
<pre><code>## 
##     setosa versicolor  virginica 
##         50         50         50</code></pre>
<p>Apparently, normalising is very important in machine learning. Using summary, we can check if datasets are too far appart</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="free-space.html#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(iris)</span></code></pre></div>
<pre><code>##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width          Species  
##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100   setosa    :50  
##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300   versicolor:50  
##  Median :5.800   Median :3.000   Median :4.350   Median :1.300   virginica :50  
##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199                  
##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800                  
##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500</code></pre>
<p>Ranges appear to be within 0.1-7.9, a factor 10 difference. Seems reasonable enough.</p>
<p>Just in case though, you usually have to create normalising functions yourself, so here’s one anyway, created by the site and broken down for me to understand it</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="free-space.html#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">normalize</span>(iris[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>])</span></code></pre></div>
<pre><code>##     Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1      0.6410256   0.4358974    0.1666667  0.01282051
## 2      0.6153846   0.3717949    0.1666667  0.01282051
## 3      0.5897436   0.3974359    0.1538462  0.01282051
## 4      0.5769231   0.3846154    0.1794872  0.01282051
## 5      0.6282051   0.4487179    0.1666667  0.01282051
## 6      0.6794872   0.4871795    0.2051282  0.03846154
## 7      0.5769231   0.4230769    0.1666667  0.02564103
## 8      0.6282051   0.4230769    0.1794872  0.01282051
## 9      0.5512821   0.3589744    0.1666667  0.01282051
## 10     0.6153846   0.3846154    0.1794872  0.00000000
## 11     0.6794872   0.4615385    0.1794872  0.01282051
## 12     0.6025641   0.4230769    0.1923077  0.01282051
## 13     0.6025641   0.3717949    0.1666667  0.00000000
## 14     0.5384615   0.3717949    0.1282051  0.00000000
## 15     0.7307692   0.5000000    0.1410256  0.01282051
## 16     0.7179487   0.5512821    0.1794872  0.03846154
## 17     0.6794872   0.4871795    0.1538462  0.03846154
## 18     0.6410256   0.4358974    0.1666667  0.02564103
## 19     0.7179487   0.4743590    0.2051282  0.02564103
## 20     0.6410256   0.4743590    0.1794872  0.02564103
## 21     0.6794872   0.4230769    0.2051282  0.01282051
## 22     0.6410256   0.4615385    0.1794872  0.03846154
## 23     0.5769231   0.4487179    0.1153846  0.01282051
## 24     0.6410256   0.4102564    0.2051282  0.05128205
## 25     0.6025641   0.4230769    0.2307692  0.01282051
## 26     0.6282051   0.3717949    0.1923077  0.01282051
## 27     0.6282051   0.4230769    0.1923077  0.03846154
## 28     0.6538462   0.4358974    0.1794872  0.01282051
## 29     0.6538462   0.4230769    0.1666667  0.01282051
## 30     0.5897436   0.3974359    0.1923077  0.01282051
## 31     0.6025641   0.3846154    0.1923077  0.01282051
## 32     0.6794872   0.4230769    0.1794872  0.03846154
## 33     0.6538462   0.5128205    0.1794872  0.00000000
## 34     0.6923077   0.5256410    0.1666667  0.01282051
## 35     0.6153846   0.3846154    0.1794872  0.01282051
## 36     0.6282051   0.3974359    0.1410256  0.01282051
## 37     0.6923077   0.4358974    0.1538462  0.01282051
## 38     0.6153846   0.4487179    0.1666667  0.00000000
## 39     0.5512821   0.3717949    0.1538462  0.01282051
## 40     0.6410256   0.4230769    0.1794872  0.01282051
## 41     0.6282051   0.4358974    0.1538462  0.02564103
## 42     0.5641026   0.2820513    0.1538462  0.02564103
## 43     0.5512821   0.3974359    0.1538462  0.01282051
## 44     0.6282051   0.4358974    0.1923077  0.06410256
## 45     0.6410256   0.4743590    0.2307692  0.03846154
## 46     0.6025641   0.3717949    0.1666667  0.02564103
## 47     0.6410256   0.4743590    0.1923077  0.01282051
## 48     0.5769231   0.3974359    0.1666667  0.01282051
## 49     0.6666667   0.4615385    0.1794872  0.01282051
## 50     0.6282051   0.4102564    0.1666667  0.01282051
## 51     0.8846154   0.3974359    0.5897436  0.16666667
## 52     0.8076923   0.3974359    0.5641026  0.17948718
## 53     0.8717949   0.3846154    0.6153846  0.17948718
## 54     0.6923077   0.2820513    0.5000000  0.15384615
## 55     0.8205128   0.3461538    0.5769231  0.17948718
## 56     0.7179487   0.3461538    0.5641026  0.15384615
## 57     0.7948718   0.4102564    0.5897436  0.19230769
## 58     0.6153846   0.2948718    0.4102564  0.11538462
## 59     0.8333333   0.3589744    0.5769231  0.15384615
## 60     0.6538462   0.3333333    0.4871795  0.16666667
## 61     0.6282051   0.2435897    0.4358974  0.11538462
## 62     0.7435897   0.3717949    0.5256410  0.17948718
## 63     0.7564103   0.2692308    0.5000000  0.11538462
## 64     0.7692308   0.3589744    0.5897436  0.16666667
## 65     0.7051282   0.3589744    0.4487179  0.15384615
## 66     0.8461538   0.3846154    0.5512821  0.16666667
## 67     0.7051282   0.3717949    0.5641026  0.17948718
## 68     0.7307692   0.3333333    0.5128205  0.11538462
## 69     0.7820513   0.2692308    0.5641026  0.17948718
## 70     0.7051282   0.3076923    0.4871795  0.12820513
## 71     0.7435897   0.3974359    0.6025641  0.21794872
## 72     0.7692308   0.3461538    0.5000000  0.15384615
## 73     0.7948718   0.3076923    0.6153846  0.17948718
## 74     0.7692308   0.3461538    0.5897436  0.14102564
## 75     0.8076923   0.3589744    0.5384615  0.15384615
## 76     0.8333333   0.3717949    0.5512821  0.16666667
## 77     0.8589744   0.3461538    0.6025641  0.16666667
## 78     0.8461538   0.3717949    0.6282051  0.20512821
## 79     0.7564103   0.3589744    0.5641026  0.17948718
## 80     0.7179487   0.3205128    0.4358974  0.11538462
## 81     0.6923077   0.2948718    0.4743590  0.12820513
## 82     0.6923077   0.2948718    0.4615385  0.11538462
## 83     0.7307692   0.3333333    0.4871795  0.14102564
## 84     0.7564103   0.3333333    0.6410256  0.19230769
## 85     0.6794872   0.3717949    0.5641026  0.17948718
## 86     0.7564103   0.4230769    0.5641026  0.19230769
## 87     0.8461538   0.3846154    0.5897436  0.17948718
## 88     0.7948718   0.2820513    0.5512821  0.15384615
## 89     0.7051282   0.3717949    0.5128205  0.15384615
## 90     0.6923077   0.3076923    0.5000000  0.15384615
## 91     0.6923077   0.3205128    0.5512821  0.14102564
## 92     0.7692308   0.3717949    0.5769231  0.16666667
## 93     0.7307692   0.3205128    0.5000000  0.14102564
## 94     0.6282051   0.2820513    0.4102564  0.11538462
## 95     0.7051282   0.3333333    0.5256410  0.15384615
## 96     0.7179487   0.3717949    0.5256410  0.14102564
## 97     0.7179487   0.3589744    0.5256410  0.15384615
## 98     0.7820513   0.3589744    0.5384615  0.15384615
## 99     0.6410256   0.3076923    0.3717949  0.12820513
## 100    0.7179487   0.3461538    0.5128205  0.15384615
## 101    0.7948718   0.4102564    0.7564103  0.30769231
## 102    0.7307692   0.3333333    0.6410256  0.23076923
## 103    0.8974359   0.3717949    0.7435897  0.25641026
## 104    0.7948718   0.3589744    0.7051282  0.21794872
## 105    0.8205128   0.3717949    0.7307692  0.26923077
## 106    0.9615385   0.3717949    0.8333333  0.25641026
## 107    0.6153846   0.3076923    0.5641026  0.20512821
## 108    0.9230769   0.3589744    0.7948718  0.21794872
## 109    0.8461538   0.3076923    0.7307692  0.21794872
## 110    0.9102564   0.4487179    0.7692308  0.30769231
## 111    0.8205128   0.3974359    0.6410256  0.24358974
## 112    0.8076923   0.3333333    0.6666667  0.23076923
## 113    0.8589744   0.3717949    0.6923077  0.25641026
## 114    0.7179487   0.3076923    0.6282051  0.24358974
## 115    0.7307692   0.3461538    0.6410256  0.29487179
## 116    0.8076923   0.3974359    0.6666667  0.28205128
## 117    0.8205128   0.3717949    0.6923077  0.21794872
## 118    0.9743590   0.4743590    0.8461538  0.26923077
## 119    0.9743590   0.3205128    0.8717949  0.28205128
## 120    0.7564103   0.2692308    0.6282051  0.17948718
## 121    0.8717949   0.3974359    0.7179487  0.28205128
## 122    0.7051282   0.3461538    0.6153846  0.24358974
## 123    0.9743590   0.3461538    0.8461538  0.24358974
## 124    0.7948718   0.3333333    0.6153846  0.21794872
## 125    0.8461538   0.4102564    0.7179487  0.25641026
## 126    0.9102564   0.3974359    0.7564103  0.21794872
## 127    0.7820513   0.3461538    0.6025641  0.21794872
## 128    0.7692308   0.3717949    0.6153846  0.21794872
## 129    0.8076923   0.3461538    0.7051282  0.25641026
## 130    0.9102564   0.3717949    0.7307692  0.19230769
## 131    0.9358974   0.3461538    0.7692308  0.23076923
## 132    1.0000000   0.4743590    0.8076923  0.24358974
## 133    0.8076923   0.3461538    0.7051282  0.26923077
## 134    0.7948718   0.3461538    0.6410256  0.17948718
## 135    0.7692308   0.3205128    0.7051282  0.16666667
## 136    0.9743590   0.3717949    0.7692308  0.28205128
## 137    0.7948718   0.4230769    0.7051282  0.29487179
## 138    0.8076923   0.3846154    0.6923077  0.21794872
## 139    0.7564103   0.3717949    0.6025641  0.21794872
## 140    0.8717949   0.3846154    0.6794872  0.25641026
## 141    0.8461538   0.3846154    0.7051282  0.29487179
## 142    0.8717949   0.3846154    0.6410256  0.28205128
## 143    0.7307692   0.3333333    0.6410256  0.23076923
## 144    0.8589744   0.3974359    0.7435897  0.28205128
## 145    0.8461538   0.4102564    0.7179487  0.30769231
## 146    0.8461538   0.3717949    0.6538462  0.28205128
## 147    0.7948718   0.3076923    0.6282051  0.23076923
## 148    0.8205128   0.3717949    0.6538462  0.24358974
## 149    0.7820513   0.4230769    0.6794872  0.28205128
## 150    0.7435897   0.3717949    0.6410256  0.21794872</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="free-space.html#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Generating a set where the the data is compensated for the lowest datapoint</span></span>
<span id="cb66-2"><a href="free-space.html#cb66-2" aria-hidden="true" tabindex="-1"></a>num<span class="ot">&lt;-</span>iris<span class="sc">$</span>Sepal.Length<span class="sc">-</span><span class="fu">min</span>(iris<span class="sc">$</span>Sepal.Length)</span>
<span id="cb66-3"><a href="free-space.html#cb66-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-4"><a href="free-space.html#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Generating the biggest difference in the dataset</span></span>
<span id="cb66-5"><a href="free-space.html#cb66-5" aria-hidden="true" tabindex="-1"></a>denom<span class="ot">&lt;-</span><span class="fu">max</span>(iris<span class="sc">$</span>Sepal.Length)<span class="sc">-</span><span class="fu">min</span>(iris<span class="sc">$</span>Sepal.Length)</span>
<span id="cb66-6"><a href="free-space.html#cb66-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-7"><a href="free-space.html#cb66-7" aria-hidden="true" tabindex="-1"></a><span class="co">#Generating the size of a datapoint relative to the maximum size of a datapoint.</span></span>
<span id="cb66-8"><a href="free-space.html#cb66-8" aria-hidden="true" tabindex="-1"></a>num<span class="sc">/</span>denom </span></code></pre></div>
<pre><code>##   [1] 0.22222222 0.16666667 0.11111111 0.08333333 0.19444444 0.30555556 0.08333333 0.19444444 0.02777778 0.16666667 0.30555556 0.13888889 0.13888889 0.00000000
##  [15] 0.41666667 0.38888889 0.30555556 0.22222222 0.38888889 0.22222222 0.30555556 0.22222222 0.08333333 0.22222222 0.13888889 0.19444444 0.19444444 0.25000000
##  [29] 0.25000000 0.11111111 0.13888889 0.30555556 0.25000000 0.33333333 0.16666667 0.19444444 0.33333333 0.16666667 0.02777778 0.22222222 0.19444444 0.05555556
##  [43] 0.02777778 0.19444444 0.22222222 0.13888889 0.22222222 0.08333333 0.27777778 0.19444444 0.75000000 0.58333333 0.72222222 0.33333333 0.61111111 0.38888889
##  [57] 0.55555556 0.16666667 0.63888889 0.25000000 0.19444444 0.44444444 0.47222222 0.50000000 0.36111111 0.66666667 0.36111111 0.41666667 0.52777778 0.36111111
##  [71] 0.44444444 0.50000000 0.55555556 0.50000000 0.58333333 0.63888889 0.69444444 0.66666667 0.47222222 0.38888889 0.33333333 0.33333333 0.41666667 0.47222222
##  [85] 0.30555556 0.47222222 0.66666667 0.55555556 0.36111111 0.33333333 0.33333333 0.50000000 0.41666667 0.19444444 0.36111111 0.38888889 0.38888889 0.52777778
##  [99] 0.22222222 0.38888889 0.55555556 0.41666667 0.77777778 0.55555556 0.61111111 0.91666667 0.16666667 0.83333333 0.66666667 0.80555556 0.61111111 0.58333333
## [113] 0.69444444 0.38888889 0.41666667 0.58333333 0.61111111 0.94444444 0.94444444 0.47222222 0.72222222 0.36111111 0.94444444 0.55555556 0.66666667 0.80555556
## [127] 0.52777778 0.50000000 0.58333333 0.80555556 0.86111111 1.00000000 0.58333333 0.55555556 0.50000000 0.94444444 0.55555556 0.58333333 0.47222222 0.72222222
## [141] 0.66666667 0.72222222 0.41666667 0.69444444 0.66666667 0.66666667 0.55555556 0.61111111 0.52777778 0.44444444</code></pre>
<p>In order to properly train the algorythm, we must separate our data into 2 groups: a “training” group and a “test” group. Apparently, usually the “training group” is 2/3rds of the dataset, while the “test group” is one third of the dataset. Important is to give the algorythm equal amounts of each test condition, if we’d split the iris set into 2/3rds and 1/3rds it’d give 50 setosa, 50 versicolor and 0 virginica which would make the algorythm not recognize virginica at all. Thus, we use setseed and sample to generate a sample.</p>

</div>
<p style="text-align: center;">
<a href="parametized-data.html"><button class="btn btn-default">Previous</button></a>
<a href="bibliography.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
